import { jsxs as _jsxs, jsx as _jsx } from "react/jsx-runtime";
import { useCallback, useEffect, useRef, useState } from 'react';
import cls from 'classnames';
import Taro from '@tarojs/taro';
import { View, Text } from '@tarojs/components';
import { getRecorderManager, RecorderEvent, logger, showToast, PlayAudio, MiniChatErrorCode, showModal, } from "../../../utils";
import { useI18n } from "../../../provider";
import { useUpdateEffect } from "../../../hooks";
import { SvgCancel } from '../../atomic/svg';
import { Spacing } from '../../atomic/spacing';
import { AudioWave } from '../../atomic/audio-wave';
import { AudioInput as AtomicAudioInput, } from '../../atomic/audio-input';
import { PermissionDeny } from './permission-deny';
import styles from './index.module.less';
const maxTime = 50;
const AudioTips = ({ isTouching, isOutside, leftDuration, triggerType }) => {
    const i18n = useI18n();
    let text = '';
    let tipType = 'touching-tips';
    if (isTouching) {
        tipType = 'touching-tips';
        if (triggerType === 'keyboard') {
            text = i18n.t('audioInputTooltipSend');
        }
        else {
            text = i18n.t('audioInputTooltipTouching');
        }
    }
    if (leftDuration && leftDuration <= 10) {
        tipType = 'time-limit-tips';
        text = i18n.t('audioInputTooltipTick', {
            duration: (_jsxs(Text, Object.assign({ className: styles['left-duration'] }, { children: [leftDuration, " \u02DD"] }))),
        }); //`${leftDuration}˝ 后自动结束录制并发送`;
    }
    if (isTouching && isOutside) {
        tipType = 'outside-tips';
        text = i18n.t('audioInputTooltipCancel');
    }
    if (!text) {
        return null;
    }
    return (_jsxs(Spacing, Object.assign({ verticalCenter: true, horizontalCenter: true, gap: 4, className: cls(styles['tips-container'], { [styles[tipType]]: true }) }, { children: [tipType === 'outside-tips' ? _jsx(SvgCancel, {}) : null, _jsx(Text, Object.assign({ className: styles['tips-text'], overflow: "ellipsis", numberOfLines: 1, maxLines: 1 }, { children: text }))] })));
};
export const AudioInput = ({ onSendAudioMessage, type = 'primary', frameEventTarget, isPcMode, disabled, focused, onAudioRecording, }) => {
    const [isTouching, setIsTouching] = useState(false);
    const [isOutside, setIsOutSide] = useState(false);
    const refIsOutSide = useRef(false);
    const refRecorderManager = useRef();
    const [triggerType, setTriggerType] = useState();
    const [volume, setVolume] = useState(0);
    const [leftDuration, setLeftDuration] = useState(50);
    const refStop = useRef();
    const i18n = useI18n();
    refIsOutSide.current = isOutside;
    logger.debug('AudioInput start', refIsOutSide.current);
    useUpdateEffect(() => {
        onAudioRecording === null || onAudioRecording === void 0 ? void 0 : onAudioRecording(isTouching);
    }, [isTouching]);
    const onTouching = useCallback((triggerTypeNew) => {
        setTriggerType(triggerTypeNew);
        setIsTouching(true);
        if (refRecorderManager.current) {
            refRecorderManager.current.destroy();
        }
        //暂停当前播放的内容
        PlayAudio.stopNow();
        let isInterrupted = false;
        refRecorderManager.current = getRecorderManager();
        logger.debug('onMouseDown onTouching');
        refStop.current = () => {
            clearTimeout(timeout);
            if (refIsOutSide.current) {
                isInterrupted = true;
            }
            setIsTouching(false);
            setIsOutSide(false);
        };
        refRecorderManager.current.on(RecorderEvent.STOP, res => {
            var _a, _b;
            logger.debug('AudioInput stop triggered', res, refIsOutSide.current);
            if (!isInterrupted && !refIsOutSide.current && res.duration) {
                if (res.duration < 1000) {
                    showToast({
                        content: i18n.t('audioInputErrorUnDetectContent'),
                        icon: 'error',
                    }, frameEventTarget);
                }
                else {
                    onSendAudioMessage === null || onSendAudioMessage === void 0 ? void 0 : onSendAudioMessage(res);
                }
            }
            (_a = refStop.current) === null || _a === void 0 ? void 0 : _a.call(refStop);
            (_b = refRecorderManager.current) === null || _b === void 0 ? void 0 : _b.destroy();
        });
        /**
         * 暂停、打断都是异常操作，直接停止。
         */
        refRecorderManager.current.on(RecorderEvent.PAUSE, () => {
            var _a;
            isInterrupted = true;
            (_a = refRecorderManager.current) === null || _a === void 0 ? void 0 : _a.stop();
            setVolume(0);
        });
        refRecorderManager.current.on(RecorderEvent.INTERRUPT, () => {
            var _a;
            isInterrupted = true;
            (_a = refRecorderManager.current) === null || _a === void 0 ? void 0 : _a.stop();
        });
        refRecorderManager.current.on(RecorderEvent.ERROR, (res) => {
            var _a, _b;
            isInterrupted = true;
            logger.error('AudioInput onError', res, res.code);
            if (res.code === MiniChatErrorCode.Audio_Permission_Denied) {
                showModal({
                    isNeedMask: true,
                    renderModal: hideModal => _jsx(PermissionDeny, { hideModal: hideModal }),
                }, frameEventTarget);
            }
            else {
                showToast({
                    content: i18n.t('audioInputErrorRecord'),
                    icon: 'error',
                }, frameEventTarget);
            }
            (_a = refStop.current) === null || _a === void 0 ? void 0 : _a.call(refStop);
            (_b = refRecorderManager.current) === null || _b === void 0 ? void 0 : _b.destroy();
        });
        refRecorderManager.current.on(RecorderEvent.VOLUME, res => {
            logger.debug('AudioInput_volume', res);
            setVolume(res.volume);
        });
        logger.debug('AudioInput register Event End');
        refRecorderManager.current.start({
            numberOfChannels: 1,
        });
        const startTime = Date.now();
        let hasVibrate = false;
        let timeout = 0;
        function tickDuration() {
            timeout = setTimeout(() => {
                var _a, _b;
                const duration = Math.ceil((Date.now() - startTime) / 1000);
                logger.debug('AudioInput duration', duration);
                if (duration > maxTime - 10) {
                    setLeftDuration(maxTime - duration);
                    if (duration >= maxTime) {
                        (_a = refRecorderManager.current) === null || _a === void 0 ? void 0 : _a.stop();
                        (_b = refStop.current) === null || _b === void 0 ? void 0 : _b.call(refStop);
                        return;
                    }
                    if (!hasVibrate) {
                        try {
                            Taro.vibrateLong();
                        }
                        catch (e) {
                            logger.error('AudioInput vibrateLong error', e);
                        }
                        hasVibrate = true;
                    }
                }
                tickDuration();
            }, 500);
        }
        tickDuration();
        setLeftDuration(maxTime);
        setIsTouching(true);
    }, []);
    useEffect(() => () => {
        var _a;
        (_a = refRecorderManager.current) === null || _a === void 0 ? void 0 : _a.destroy();
    }, []);
    const audioWaveType = type === 'default' ? 'default' : isOutside ? 'warning' : 'primary';
    const realFocused = focused || !isPcMode;
    const buttonText = disabled
        ? i18n.t('audioInputDisabledText')
        : realFocused
            ? isPcMode
                ? i18n.t('audioInputPcText')
                : i18n.t('audioInputMobileText')
            : isPcMode
                ? i18n.t('unFocusInputText')
                : i18n.t('audioInputMobileText');
    return (_jsxs(View, Object.assign({ className: cls(styles.container, {
            [styles.touching]: isTouching && !disabled,
            [styles.outside]: isOutside && !disabled,
            [styles[type]]: true,
            [styles.focused]: realFocused && !disabled,
        }) }, { children: [_jsx(AudioTips, { isTouching: isTouching, isOutside: isOutside, leftDuration: leftDuration, triggerType: triggerType }), _jsxs(AtomicAudioInput, Object.assign({ onTouching: onTouching, onOutside: isOutsideNew => {
                    logger.debug('AudioInput OutSide touch', isOutsideNew);
                    setIsOutSide(isOutsideNew);
                }, onEnd: () => {
                    var _a, _b;
                    logger.debug('AudioInput recording onEnd');
                    (_a = refRecorderManager.current) === null || _a === void 0 ? void 0 : _a.stop();
                    (_b = refStop.current) === null || _b === void 0 ? void 0 : _b.call(refStop);
                }, frameEventTarget: frameEventTarget, isPcMode: isPcMode, className: styles.audio, disabled: disabled }, { children: [_jsx(Text, Object.assign({ overflow: "ellipsis", numberOfLines: 1, maxLines: 1, userSelect: false, className: styles.text }, { children: buttonText })), isTouching ? (_jsx(AudioWave, { type: audioWaveType, size: "medium", volumeNumber: volume * 5, className: styles.wave })) : null] }))] })));
};
//# sourceMappingURL=index.js.map